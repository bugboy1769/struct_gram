{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Table-to-Graph Learning Pipeline\n",
    "## Table-Question Alignment with InfoNCE Loss\n",
    "\n",
    "This notebook trains a contrastive learning model that:\n",
    "- Encodes table structures as graph embeddings using GNN\n",
    "- Encodes natural language questions using SentenceTransformer\n",
    "- Aligns table graphs with matching questions using InfoNCE loss\n",
    "- Enables semantic table retrieval given a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q sentence-transformers torch-geometric scikit-learn tqdm\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for checkpoint storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/contrastive_table2graph_checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify required files are uploaded\n",
    "import os\n",
    "\n",
    "required_files = ['contrastive_table2graph.py', 'gcn_conv.py']\n",
    "for f in required_files:\n",
    "    if os.path.exists(f'/content/{f}'):\n",
    "        print(f\"✓ {f} found\")\n",
    "    else:\n",
    "        print(f\"✗ {f} MISSING - please upload!\")\n",
    "\n",
    "# Check for data directory\n",
    "if os.path.exists('/content/data'):\n",
    "    csv_count = len([f for f in os.listdir('/content/data') if f.endswith('.csv')])\n",
    "    print(f\"✓ data/ folder found with {csv_count} CSV files\")\n",
    "else:\n",
    "    print(\"✗ data/ folder MISSING - please upload CSV files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('/content')\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\nimport json\n\n# Import pipeline components\nfrom contrastive_table2graph import (\n    DataProcessor,\n    ColumnContentExtractor,\n    LightweightFeatureTokenizer,\n    RelationshipGenerator,\n    SemanticLabelGenerator,\n    GraphBuilder,\n    QuestionEncoder,\n    ContrastiveGNNEncoder,\n    AttentionPooling,\n    InfoNCELoss,\n    QuestionGenerator,\n    TableQuestionDataset,\n    collate_fn,\n    create_dataloader\n)\n\nfrom torch.utils.data import random_split\n\nprint(\"✓ Pipeline components imported successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"✓ Random seed set to 42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_tables_from_directory(data_dir, max_tables=None, max_rows=500):\n    \"\"\"\n    Load CSV tables from directory.\n    \n    Args:\n        data_dir: Path to directory with CSV files\n        max_tables: Max number of tables to load (None = all)\n        max_rows: Max rows per table (for memory management)\n    \n    Returns:\n        List of (table_name, DataFrame) tuples\n    \"\"\"\n    data_path = Path(data_dir)\n    csv_files = sorted(data_path.glob('*.csv'))\n    \n    if max_tables is not None:\n        csv_files = csv_files[:max_tables]\n    \n    print(f\"Found {len(csv_files)} CSV files\\n\")\n    \n    tables = []\n    data_processor = DataProcessor()\n    \n    for csv_file in csv_files:\n        table_name = csv_file.stem\n        \n        try:\n            df = pd.read_csv(csv_file, nrows=max_rows, low_memory=False)\n            \n            # Skip invalid tables\n            if len(df.columns) < 2:\n                print(f\"  ✗ {table_name}: < 2 columns\")\n                continue\n            \n            if len(df) == 0:\n                print(f\"  ✗ {table_name}: empty table\")\n                continue\n            \n            # PHASE 6: Assign table name for tracking\n            df.name = table_name\n            \n            tables.append((table_name, df))\n            print(f\"  ✓ {table_name}: {df.shape[0]} × {df.shape[1]}\")\n            \n        except Exception as e:\n            print(f\"  ✗ {table_name}: {e}\")\n            continue\n    \n    return tables\n\n# Load tables\nDATA_DIR = '/content/data'\ntables = load_tables_from_directory(DATA_DIR, max_tables=50, max_rows=500)\n\nprint(f\"\\n✓ Loaded {len(tables)} tables successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display table statistics\n",
    "print(\"Table Statistics:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total_rows = sum(df.shape[0] for _, df in tables)\n",
    "total_cols = sum(df.shape[1] for _, df in tables)\n",
    "avg_rows = total_rows / len(tables)\n",
    "avg_cols = total_cols / len(tables)\n",
    "\n",
    "print(f\"Total tables: {len(tables)}\")\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "print(f\"Total columns: {total_cols}\")\n",
    "print(f\"Avg rows per table: {avg_rows:.1f}\")\n",
    "print(f\"Avg columns per table: {avg_cols:.1f}\")\n",
    "\n",
    "print(\"\\nSample Tables:\")\n",
    "for name, df in tables[:5]:\n",
    "    print(f\"  {name}: {df.shape[0]} × {df.shape[1]} - {list(df.columns[:3])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Initializing pipeline components...\\n\")\n\n# Core components\ncontent_extractor = ColumnContentExtractor()\nprint(\"✓ ColumnContentExtractor\")\n\n# PHASE 6: Enable column name semantics (512 -> 896 dimensions)\nfeature_tokenizer = LightweightFeatureTokenizer(\n    embedding_strategy='hybrid',\n    include_column_names=True\n)\nprint(\"✓ LightweightFeatureTokenizer\")\nprint(f\"  - Node feature dimension: {feature_tokenizer.feature_dim}\")\n\nrelationship_generator = RelationshipGenerator()\nprint(\"✓ RelationshipGenerator\")\n\nsemantic_label_generator = SemanticLabelGenerator()\nprint(\"✓ SemanticLabelGenerator\")\nprint(f\"  - 34 semantic relationship types\")\n\n# Graph converter (converts tables to PyG graphs)\npyg_converter = GraphBuilder(\n    content_extractor=content_extractor,\n    feature_tokenizer=feature_tokenizer,\n    relationship_generator=relationship_generator,\n    semantic_label_generator=semantic_label_generator,\n    mode='train'\n)\nprint(\"✓ GraphBuilder (PyG converter)\")\n\n# Question generator\nquestion_generator = QuestionGenerator(\n    semantic_label_generator=semantic_label_generator\n)\nprint(\"✓ QuestionGenerator\")\nprint(f\"  - 6 pattern-based categories with 12 templates each\")\n\nprint(\"\\n✓ All components initialized\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Question-Table Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating question-table pairs...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract DataFrames\n",
    "table_dfs = [df for _, df in tables]\n",
    "\n",
    "# Generate questions (only positive pairs - negatives from in-batch sampling)\n",
    "question_data = question_generator.generate_dataset(\n",
    "    tables=table_dfs,\n",
    "    relationship_generator=relationship_generator,\n",
    "    num_per_table=20  # 20 questions per table\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(question_data)} question-table pairs\")\n",
    "print(f\"  - All positive pairs (label=1)\")\n",
    "print(f\"  - {len(question_data) / len(table_dfs):.1f} questions per table (avg)\")\n",
    "print(f\"  - In-batch negatives will be used during training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display sample questions\nprint(\"\\nSample Questions:\")\nprint(\"=\" * 70)\n\nfor i, q_data in enumerate(question_data[:5]):\n    print(f\"\\n{i+1}. Question: {q_data['question']}\")\n    print(f\"   Table name: {q_data.get('table_name', 'unknown')}\")\n    print(f\"   Table shape: {q_data['table'].shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating datasets...\\n\")\n",
    "\n",
    "# Create full dataset\n",
    "full_dataset = TableQuestionDataset(\n",
    "    question_data=question_data,\n",
    "    data_processor=DataProcessor(),\n",
    "    pyg_converter=pyg_converter\n",
    ")\n",
    "\n",
    "# Train/val split (80/20)\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"Dataset Splits:\")\n",
    "print(f\"  Train: {len(train_dataset)} pairs ({len(train_dataset)/total_size*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_dataset)} pairs ({len(val_dataset)/total_size*100:.1f}%)\")\n",
    "print(f\"  Total: {total_size} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = create_dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0  # Colab compatibility\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders Created:\")\n",
    "print(f\"  Train batches: {len(train_loader)} (batch_size={BATCH_SIZE})\")\n",
    "print(f\"  Val batches:   {len(val_loader)} (batch_size={BATCH_SIZE})\")\n",
    "print(f\"\\n  In each batch:\")\n",
    "print(f\"    - {BATCH_SIZE} positive pairs (question[i] ↔ table[i])\")\n",
    "print(f\"    - {BATCH_SIZE * (BATCH_SIZE - 1)} in-batch negative pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device detection\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nInitializing models...\\n\")\nprint(\"=\" * 70)\n\n# Question encoder (frozen SentenceTransformer)\nquestion_encoder = QuestionEncoder(\n    model_name='all-mpnet-base-v2',\n    freeze=True\n)\nprint(\"✓ QuestionEncoder: all-mpnet-base-v2 (frozen)\")\nprint(f\"  Output dim: {question_encoder.output_dim}\")\n\n# CRITICAL FIX: Changed hidden_dim from 256 to 768 to prevent information bottleneck\n# Previous: 896 → 256 destroyed 71% of features\n# Now: 896 → 768 preserves 86% of features and matches question space dimension\ngraph_encoder = ContrastiveGNNEncoder(\n    node_dim=896,\n    hidden_dim=768,  # ← FIXED: Was 256, now 768 to preserve information\n    output_dim=768,\n    num_layers=2\n)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in graph_encoder.parameters())\ntrainable_params = sum(p.numel() for p in graph_encoder.parameters() if p.requires_grad)\n\nprint(\"\\n✓ ContrastiveGNNEncoder: 2-layer GNN\")\nprint(f\"  Node dim: 896 (512 statistical + 384 column name semantics)\")\nprint(f\"  Hidden dim: 768 (FIXED: was 256, preserves features)\")\nprint(f\"  Output dim: 768\")\nprint(f\"  Total params: {total_params:,}\")\nprint(f\"  Trainable params: {trainable_params:,}\")\n\n# Move to device\ngraph_encoder = graph_encoder.to(device)\nquestion_encoder.encoder = question_encoder.encoder.to(device)\n\nprint(f\"\\n✓ Models moved to {device}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Loss function and optimizer\nloss_fn = InfoNCELoss(temperature=0.07)\nprint(f\"✓ InfoNCELoss: temperature=0.07\")\n\n# Increased learning rate from 1e-4 to 5e-4 for better convergence\noptimizer = torch.optim.AdamW(\n    graph_encoder.parameters(),\n    lr=5e-4,\n    weight_decay=0.01\n)\nprint(f\"✓ AdamW optimizer: lr=5e-4, weight_decay=0.01\")\n\n# Add cosine annealing scheduler for smooth convergence\n# Reduces LR from 5e-4 to near-zero over num_epochs\nnum_training_steps = len(train_loader) * 50  # Total steps across all epochs\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=num_training_steps,\n    eta_min=1e-6\n)\nprint(f\"✓ CosineAnnealingLR scheduler: T_max={num_training_steps}, eta_min=1e-6\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'num_epochs': 50,\n",
    "    'gradient_clip': 1.0,\n",
    "    'print_every': 1,\n",
    "    'save_every': 10,\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 70)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_recall_at_k(graph_embeddings, question_embeddings, k=1):\n    \"\"\"Compute Recall@K for retrieval.\"\"\"\n    num_questions = question_embeddings.size(0)\n    \n    # Similarity matrix\n    similarity = torch.matmul(question_embeddings, graph_embeddings.T)\n    \n    # Top-k indices\n    _, top_k_indices = torch.topk(similarity, k=min(k, similarity.size(1)), dim=1)\n    \n    # Check if correct graph in top-k\n    correct_indices = torch.arange(num_questions, device=graph_embeddings.device).unsqueeze(1)\n    \n    # Check if correct index is in top-k predictions\n    matches = (top_k_indices == correct_indices).any(dim=1)\n    recall_at_k = matches.float().mean().item()\n    \n    return recall_at_k\n\ndef train_epoch(graph_encoder, question_encoder, loss_fn, optimizer, scheduler, train_loader, device, config):\n    \"\"\"Train for one epoch.\"\"\"\n    graph_encoder.train()\n    epoch_loss = 0.0\n    num_batches = 0\n    \n    pbar = tqdm(train_loader, desc=\"Training\")\n    \n    # PHASE 6: Handle dict return from collate_fn\n    for batch in pbar:\n        batched_graphs = batch['graphs'].to(device)\n        questions = batch['questions']\n        labels = batch['labels']\n        # table_names = batch['table_names']  # Available if needed\n        \n        # Forward pass\n        graph_embeddings = graph_encoder(\n            batched_graphs,\n            batch=batched_graphs.batch\n        )\n        \n        question_embeddings = question_encoder(questions)\n        question_embeddings = question_embeddings.to(device)\n        \n        # Compute loss\n        loss = loss_fn(graph_embeddings, question_embeddings)\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(\n            graph_encoder.parameters(),\n            max_norm=config['gradient_clip']\n        )\n        optimizer.step()\n        scheduler.step()  # Update learning rate after each batch\n        \n        epoch_loss += loss.item()\n        num_batches += 1\n        \n        # Show current LR in progress bar\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{current_lr:.2e}'})\n    \n    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n    return avg_loss\n\n@torch.no_grad()\ndef validate(graph_encoder, question_encoder, loss_fn, val_loader, device):\n    \"\"\"Validate on validation set.\"\"\"\n    graph_encoder.eval()\n    total_loss = 0.0\n    num_batches = 0\n    \n    all_graph_embeddings = []\n    all_question_embeddings = []\n    \n    # PHASE 6: Handle dict return from collate_fn\n    for batch in val_loader:\n        batched_graphs = batch['graphs'].to(device)\n        questions = batch['questions']\n        labels = batch['labels']\n        \n        # Forward pass\n        graph_embeddings = graph_encoder(\n            batched_graphs,\n            batch=batched_graphs.batch\n        )\n        \n        question_embeddings = question_encoder(questions)\n        question_embeddings = question_embeddings.to(device)\n        \n        # Compute loss\n        loss = loss_fn(graph_embeddings, question_embeddings)\n        total_loss += loss.item()\n        num_batches += 1\n        \n        # Store for recall calculation\n        all_graph_embeddings.append(graph_embeddings)\n        all_question_embeddings.append(question_embeddings)\n    \n    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n    \n    # Compute Recall@K\n    graph_embs = torch.cat(all_graph_embeddings, dim=0)\n    question_embs = torch.cat(all_question_embeddings, dim=0)\n    \n    recall_1 = compute_recall_at_k(graph_embs, question_embs, k=1)\n    recall_5 = compute_recall_at_k(graph_embs, question_embs, k=5)\n    \n    return {\n        'loss': avg_loss,\n        'recall@1': recall_1,\n        'recall@5': recall_5\n    }\n\nprint(\"✓ Training utilities defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Starting training...\\n\")\nprint(\"=\" * 70)\n\nhistory = {\n    'epoch': [],\n    'train_loss': [],\n    'val_loss': [],\n    'val_recall@1': [],\n    'val_recall@5': [],\n    'learning_rate': [],\n    'time': []\n}\n\nbest_recall = 0.0\n\nfor epoch in range(CONFIG['num_epochs']):\n    epoch_start = time.time()\n    \n    # Train (now with scheduler)\n    train_loss = train_epoch(\n        graph_encoder, question_encoder, loss_fn, optimizer, scheduler,\n        train_loader, device, CONFIG\n    )\n    \n    # Validate\n    val_metrics = validate(\n        graph_encoder, question_encoder, loss_fn, val_loader, device\n    )\n    \n    epoch_time = time.time() - epoch_start\n    current_lr = optimizer.param_groups[0]['lr']\n    \n    # Record history\n    history['epoch'].append(epoch + 1)\n    history['train_loss'].append(float(train_loss))\n    history['val_loss'].append(float(val_metrics['loss']))\n    history['val_recall@1'].append(float(val_metrics['recall@1']))\n    history['val_recall@5'].append(float(val_metrics['recall@5']))\n    history['learning_rate'].append(float(current_lr))\n    history['time'].append(float(epoch_time))\n    \n    # Print progress\n    if (epoch + 1) % CONFIG['print_every'] == 0:\n        print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n        print(f\"  Train Loss: {train_loss:.4f}\")\n        print(f\"  Val Loss:   {val_metrics['loss']:.4f}\")\n        print(f\"  Recall@1:   {val_metrics['recall@1']:.3f}\")\n        print(f\"  Recall@5:   {val_metrics['recall@5']:.3f}\")\n        print(f\"  LR:         {current_lr:.2e}\")\n        print(f\"  Time:       {epoch_time:.2f}s\")\n    \n    # Save best model\n    if val_metrics['recall@1'] > best_recall:\n        best_recall = val_metrics['recall@1']\n        torch.save({\n            'epoch': epoch + 1,\n            'graph_encoder_state': graph_encoder.state_dict(),\n            'optimizer_state': optimizer.state_dict(),\n            'scheduler_state': scheduler.state_dict(),\n            'metrics': val_metrics,\n        }, f\"{CHECKPOINT_DIR}/best_model.pt\")\n        print(f\"  ✓ New best model saved (Recall@1: {best_recall:.3f})\")\n    \n    # Periodic checkpoint\n    if (epoch + 1) % CONFIG['save_every'] == 0:\n        torch.save({\n            'epoch': epoch + 1,\n            'graph_encoder_state': graph_encoder.state_dict(),\n            'optimizer_state': optimizer.state_dict(),\n            'scheduler_state': scheduler.state_dict(),\n            'metrics': val_metrics,\n        }, f\"{CHECKPOINT_DIR}/checkpoint_epoch_{epoch+1}.pt\")\n        print(f\"  ✓ Checkpoint saved\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(f\"Training Complete!\")\nprint(f\"Best Recall@1: {best_recall:.3f}\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['epoch'], history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['epoch'], history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('InfoNCE Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall curves\n",
    "axes[1].plot(history['epoch'], history['val_recall@1'], 'g-', label='Recall@1', linewidth=2)\n",
    "axes[1].plot(history['epoch'], history['val_recall@5'], 'm-', label='Recall@5', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Recall', fontsize=12)\n",
    "axes[1].set_title('Validation Recall Metrics', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHECKPOINT_DIR}/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Training curves saved to {CHECKPOINT_DIR}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Table Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(f\"{CHECKPOINT_DIR}/best_model.pt\", map_location=device)\n",
    "graph_encoder.load_state_dict(checkpoint['graph_encoder_state'])\n",
    "graph_encoder.eval()\n",
    "\n",
    "print(f\"✓ Loaded best model (Epoch {checkpoint['epoch']})\")\n",
    "print(f\"  Recall@1: {checkpoint['metrics']['recall@1']:.3f}\")\n",
    "print(f\"  Recall@5: {checkpoint['metrics']['recall@5']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for all tables\n",
    "print(\"\\nGenerating embeddings for all tables...\")\n",
    "\n",
    "table_embeddings_list = []\n",
    "table_names_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for table_name, df in tqdm(tables, desc=\"Encoding tables\"):\n",
    "        # Convert to graph\n",
    "        pyg_data = pyg_converter.convert_table(df)\n",
    "        pyg_data = pyg_data.to(device)\n",
    "        \n",
    "        # Encode\n",
    "        graph_emb = graph_encoder(pyg_data, batch=None)\n",
    "        \n",
    "        table_embeddings_list.append(graph_emb.cpu())\n",
    "        table_names_list.append(table_name)\n",
    "\n",
    "table_embeddings = torch.cat(table_embeddings_list, dim=0)\n",
    "print(f\"✓ Encoded {len(tables)} tables: {table_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_tables(query_question, top_k=5):\n",
    "    \"\"\"\n",
    "    Given a question, retrieve top-k most relevant tables.\n",
    "    \n",
    "    Args:\n",
    "        query_question: Natural language question\n",
    "        top_k: Number of tables to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of (table_name, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # Encode question\n",
    "    with torch.no_grad():\n",
    "        question_emb = question_encoder([query_question])\n",
    "        question_emb = question_emb.to(device)\n",
    "    \n",
    "    # Compute similarities\n",
    "    table_embs_device = table_embeddings.to(device)\n",
    "    similarities = torch.matmul(question_emb, table_embs_device.T).squeeze()\n",
    "    \n",
    "    # Get top-k\n",
    "    top_k_scores, top_k_indices = torch.topk(similarities, k=min(top_k, len(tables)))\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(top_k_scores, top_k_indices):\n",
    "        results.append({\n",
    "            'table_name': table_names_list[idx],\n",
    "            'similarity': score.item(),\n",
    "            'table_shape': tables[idx][1].shape\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Retrieval function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# PHASE 6: Test retrieval with table-level questions\ntest_questions = [\n    \"Which table links multiple entities together?\",\n    \"Which table tracks time-ordered events?\",\n    \"Which table stores quantitative measurements?\",\n    \"Which table provides lookup codes and descriptions?\",\n    \"Which table records transactional activities?\",\n    \"Which table provides descriptive attributes?\"\n]\n\nfor query in test_questions:\n    print(\"\\n\" + \"=\" * 70)\n    print(f\"Query: {query}\")\n    print(\"=\" * 70)\n    \n    results = retrieve_tables(query, top_k=5)\n    \n    print(\"\\nTop 5 Retrieved Tables:\")\n    for i, result in enumerate(results):\n        print(f\"{i+1}. {result['table_name']}\")\n        print(f\"   Similarity: {result['similarity']:.4f}\")\n        print(f\"   Shape: {result['table_shape']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "with open(f'{CHECKPOINT_DIR}/training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"✓ Training history saved\")\n",
    "\n",
    "# Save final metrics\n",
    "final_metrics = {\n",
    "    'best_recall@1': best_recall,\n",
    "    'final_epoch': CONFIG['num_epochs'],\n",
    "    'num_tables': len(tables),\n",
    "    'num_questions': len(question_data),\n",
    "    'train_size': len(train_dataset),\n",
    "    'val_size': len(val_dataset),\n",
    "    'model_params': total_params,\n",
    "}\n",
    "\n",
    "with open(f'{CHECKPOINT_DIR}/final_metrics.json', 'w') as f:\n",
    "    json.dump(final_metrics, f, indent=2)\n",
    "\n",
    "print(f\"✓ Final metrics saved\")\n",
    "print(f\"\\n✓ All results exported to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "### Training Complete!\n",
    "\n",
    "**Model**: Contrastive GNN for Table-Question Alignment\n",
    "- Architecture: 2-layer GNN + Attention Pooling + Projection Head\n",
    "- Loss: InfoNCE with in-batch negatives\n",
    "- Question Encoder: all-mpnet-base-v2 (frozen)\n",
    "- Graph Encoder: Trainable (~200K parameters)\n",
    "\n",
    "**Saved Artifacts**:\n",
    "- `best_model.pt` - Best model checkpoint\n",
    "- `checkpoint_epoch_*.pt` - Periodic checkpoints\n",
    "- `training_curves.png` - Loss and recall curves\n",
    "- `training_history.json` - Complete training history\n",
    "- `final_metrics.json` - Summary statistics\n",
    "\n",
    "**Usage**:\n",
    "- Load best model for inference\n",
    "- Use `retrieve_tables(question)` to find relevant tables\n",
    "- Recall@1 measures exact match accuracy\n",
    "- Recall@5 measures if correct table is in top-5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}